oice Assistant STT/TTS Solutions – Technical Evaluation
Introduction
Building a cloud-based voice assistant with mobile access involves integrating Speech-to-Text (STT), Text-to-Speech (TTS), and wake word detection into a seamless system. The assistant should run on an Azure backend (Python) with a React front-end, and be triggered via NFC launches on both iOS (App Clip) and Android. This report evaluates the leading STT and TTS solutions for such an assistant, focusing on continuous dialogue, low-latency streaming, multilingual and emotional speech support, and integration complexity. We compare solutions including OpenAI’s latest voice models, Deepgram, AssemblyAI, Play.ht, Coqui (Mozilla/Mycroft), Whisper-based setups (Whisper/WhisperX), “Sesame” (open multilingual models), and other high-quality options from GitHub/HuggingFace. Key requirements such as wake-word activation, interrupting TTS on user barge-in, stop-word commands (“stop”, “cancel”), and storing conversations in MongoDB/Pinecone for each user are addressed.
Requirements and Challenges
Wake Word Activation: The assistant should remain passive until a wake word (e.g. “Hey Assistant”) is detected, then start listening. Wake word detection must be accurate and on-device (for privacy and latency) so the system can transition from passive to active listening​
rhasspy.readthedocs.io
.
Continuous Dialogue & Multi-Turn Context: After activation, the assistant should handle multiple back-and-forth turns without requiring the wake word before each utterance. This demands maintaining context and keeping the microphone open between turns (with proper voice activity detection to know when the user is speaking or has finished). The system must handle natural, contextual conversation​
developer.nvidia.com
.
Low Latency, Real-Time Streaming: Both STT and TTS should operate with minimal latency – ideally processing in real-time or faster. Even an added 200 ms delay is perceptible to users and can degrade the experience​
developer.nvidia.com
. The assistant should feel instantaneous in transcribing speech and responding. Streaming protocols (WebSocket or WebRTC) are needed for live audio transfer and incremental transcription.
Interruption (Barge-in) Support: The user should be able to interrupt the assistant’s speech (TTS) by speaking the wake word again or using a stop command. The system must detect the user’s voice over the output audio and halt TTS output promptly (a feature known as “barge-in”). This is challenging because the microphone may pick up the assistant’s own voice; solutions include half-duplex audio or echo cancellation when using full-duplex audio streams.
“Stop” Command Handling: A specific voice command like “Stop” or “Cancel” should immediately stop the assistant’s action or speech. This requires the STT/voice-command parser to quickly recognize certain keywords (perhaps via a lightweight keyword spotter running in parallel) even when TTS is playing.
High STT Accuracy: Transcription must be very accurate (low Word Error Rate, WER) for natural conversation. Domain-specific terms, proper nouns, and multi-lingual input (English, Dutch, etc.) should be handled. The chosen STT should approach human-level accuracy – modern models can achieve ~92–94% accuracy on English (WER ~6–8%)​
assemblyai.com
​
assemblyai.com
. Misrecognitions could trigger wrong actions or responses, so a state-of-the-art STT is needed.
Emotional, Multilingual TTS: The assistant’s voice should sound natural and expressive, with support for multiple languages (English, Dutch, etc.) and emotional intonations (e.g. cheerful, sad, excited) to convey nuance. TTS voices with controllable style (via SSML or APIs) are preferred so the assistant can shift tone. For instance, Azure Neural TTS offers voices that can speak in tones like cheerful, angry, sad, excited, etc​
voicebot.ai
. Open-source options like Coqui/Mimic3 also provide many voices and languages offline​
voicebot.ai
.
Resource and Cost Constraints: Daily usage is expected to be 2–5 hours per user, which is substantial. The solution should be cost-effective at scale – open-source or self-hosted components might be preferable to avoid extremely high API fees for heavy usage, as long as they can run efficiently on Azure infrastructure. Cloud API services should be evaluated for pricing per hour or per million characters.
Integration & Data Pipeline: The system needs to integrate STT->LLM->TTS in a pipeline. Audio from the user goes to STT, text is sent to an LLM or task dispatcher (like OpenAI GPT-4, Mistral, or a custom action handler), a response text is generated, then TTS converts it to voice. All user queries and assistant responses (and possibly embeddings of them) should be stored: MongoDB for raw logs and Pinecone (vector DB) for semantic embeddings to enable long-term memory or context retrieval in future queries. Each user may have a profile with a “private mode” vs “work mode” that could use different STT/TTS settings or vocabularies, meaning the system might switch models or lexicons per user context. Ensuring compatibility with Azure deployment (e.g. containerizing the STT/TTS services or using Azure Cognitive Services) and the front-end (React web app triggered by NFC) is also crucial.
The following sections compare candidate STT and TTS solutions against these requirements, including their latency, accuracy, language support, ability to handle interruptions, and integration complexity. We also consider wake-word detection engines separately. Finally, we recommend the top 2–3 combinations of STT+TTS (and wake word) and provide code examples for integration in the Python backend and React front-end.
Speech-to-Text (STT) Solutions Comparison
Modern STT options range from cloud APIs to open-source models. We evaluate each on accuracy (WER), latency (especially streaming), multilingual capabilities, and how well they support continuous dialogue and integration. 1. OpenAI Whisper (and Whisper-based APIs): OpenAI’s Whisper is an open-source Transformer-based ASR that achieves near state-of-the-art accuracy across many languages. Whisper Large-v2 has ~5-8% WER on English benchmarks (and ~7.9% WER on a broad set of English test data)​
assemblyai.com
​
assemblyai.com
, approaching AssemblyAI’s top model. It’s trained on 98+ languages, so it can transcribe English, Dutch, and many others out-of-the-box (Meta’s SeamlessM4T similarly supports ~100 languages for speech recognition)​
about.fb.com
. Multilingual support is a strong advantage – Whisper can seamlessly handle code-switching or accents. It also provides automatic punctuation and some degree of diarization (speaker differentiation) in the latest versions.
Latency & Streaming: Whisper’s main drawback is latency for real-time use. The open model transcribes audio in chunks (up to 30 seconds) rather than truly streaming. However, it can be integrated in a pseudo-streaming manner by using shorter segments (e.g. 5-second windows) or leveraging projects like WhisperX that provide word-level timestamps and alignment post hoc. WhisperX can refine timestamps and speaker labels​
assemblyai.com
, but it operates after transcription, so it’s more useful for aligning transcripts than for live use. For real-time dialogue, one could run Whisper in smaller increments or use the Whisper API from OpenAI. OpenAI’s cloud API for Whisper accepts audio and returns text; while highly accurate, it currently does not support real-time WebSocket streaming (audio must be sent as a whole or chunked and polled). This adds a bit of complexity to get low latency – the app would have to continuously send partial recordings and get partial transcripts. Some third-party wrappers or forked versions of Whisper support streaming decoding, but they may not be as straightforward.
Integration: Running Whisper large locally on Azure requires a GPU for real-time transcription (it’s heavy – ~1.5 billion params). On an Azure VM with GPU, Whisper large can transcribe ~ realtime or slightly slower; smaller models (medium or small) run faster but with lower accuracy. For a cloud API approach, OpenAI’s Whisper API is inexpensive (roughly $0.006 per minute of audio, which is ~$0.36 per hour) and very accurate in many languages, making it a cost-effective choice​
assemblyai.com
​
assemblyai.com
. Integration via Python is easy (just an HTTP request with the audio file). However, lack of true streaming means latency might be a few seconds for each utterance, which could hinder continuous conversation flow.
Special Features: Whisper is robust to different accents and even can transcribe Dutch speech reasonably well (Whisper was trained on Dutch examples, whereas some commercial engines may be English-optimized). It doesn’t inherently support wake-word (that’s separate) or custom vocabulary boosting, though its accuracy is high enough that likely no extra tuning is needed for general use. It will transcribe everything, including someone saying “stop” – that can be detected in the text. One nice feature: it can detect the language spoken and transcribe accordingly, which is useful if the assistant needs to handle multi-language input dynamically.
2. OpenAI “Realtime” Voice (Whisper + New TTS): OpenAI has also introduced an end-to-end voice system for ChatGPT (allowing users to talk and hear responses). While details are limited, it likely combines Whisper for STT and a new neural codec TTS model for the voice output. The OpenAI realtime guide hints at streaming transcription and response. If this system becomes available via API, it could simplify integration (one vendor for both STT and TTS). Whisper’s accuracy and the new TTS’s naturalness (which is extremely high – nearly human-like in intonation) would cover core needs. However, as of now, OpenAI’s voice output is only integrated into specific products (ChatGPT mobile) and not an open API. We might have to exclude it from immediate options, but keep an eye on it as a future upgrade – OpenAI’s TTS voices are multilingual to an extent (the demo voices speak English; it’s unclear if they support Dutch yet) and are highly expressive. For now, we consider separate STT and TTS components. 3. AssemblyAI “Universal” STT: AssemblyAI offers a cloud STT API that boasts market-leading accuracy. Their latest Universal model is shown to slightly outperform Whisper on many tests​
assemblyai.com
​
assemblyai.com
. For example, on a mixed dataset, AssemblyAI’s model achieved ~6.6% WER in English vs OpenAI Whisper’s ~7.9%​
assemblyai.com
​
assemblyai.com
 – roughly 40% fewer errors than some competitors according to their benchmarks​
assemblyai.com
. It also excels at formatting (punctuation, proper nouns, etc.) and handling alphanumeric sequences correctly. AssemblyAI supports real-time streaming via WebSockets: you can stream audio and receive live transcripts with low latency. This makes it well-suited for continuous dialogue – partial results can be shown or used to detect “stop” commands immediately.
Multilingual: AssemblyAI’s Universal model is multilingual (English, Spanish, German tested, with high accuracy in each​
assemblyai.com
). It likely supports Dutch as well (though we’d confirm their language list – given their focus on European languages, Dutch is probably supported or coming). Their API documentation should list supported languages; if Dutch isn’t explicitly supported, Whisper or Deepgram might be fallback for Dutch.
Latency: The streaming API returns interim results typically within a few hundred milliseconds of speech, and final results shortly after the user stops talking. This meets the low-latency requirement. AssemblyAI has an improved streaming model as of early 2025 which should be very responsive.
Integration & Pricing: Integration is straightforward via their Python SDK or REST. However, AssemblyAI’s pricing is enterprise-tier – not publicly listed in detail (you need to contact for high volumes)​
assemblyai.com
. In the past it was around $0.025 per minute for realtime, but it may have changed. Assuming similar pricing to competitors, it could be on the order of $1-2 per hour of audio. Given heavy daily usage per user, cost can add up. But AssemblyAI might offer volume discounts. The integration complexity is low (just an API call or opening a WebSocket). One con: it’s not open-source, so you rely on their service uptime and can’t self-host. However, data privacy is handled (they claim security focus and even offer on-prem or private cloud deployment for enterprise). For our Azure deployment, calling out to AssemblyAI’s API is fine as long as latency to their servers (likely US/EU endpoints) is low.
Features: AssemblyAI also provides extra speech understanding features (like topic detection, sentiment, entity detection) – not required for our use-case but could be interesting for extended functionality. Wake-word still needs to be separate (AssemblyAI doesn’t provide KWS). For stop-words, we can simply monitor transcripts for the word “stop” and then act accordingly.
4. Deepgram: Deepgram is another popular STT API known for its streaming capabilities and scalability. It uses end-to-end deep learning models under the hood (they have a model called Nova 2 for English). Accuracy-wise, Deepgram is on par with big tech: in AssemblyAI’s test, Deepgram’s English accuracy ~91.0% (about 9.0% WER)​
assemblyai.com
​
assemblyai.com
 – slightly below Whisper and AssemblyAI on that particular benchmark, but still quite good. Deepgram supports many languages (over a dozen) including English, Spanish, German, Dutch, and more – their models for Spanish and German show strong accuracy too​
assemblyai.com
​
assemblyai.com
, implying Dutch is likely supported (Deepgram’s docs list Dutch as beta support).
Latency & Streaming: Deepgram provides a streaming WebSocket API that is known for very low latency. They claim sub-300ms latency for generating transcripts after speech is spoken (and can do real-time transcription on audio streams). This makes it excellent for continuous dialogue. The assistant can open a WebSocket to Deepgram and get interim transcripts as the user speaks. As soon as the user says “stop”, the partial transcript would show that, allowing immediate action.
Integration: Deepgram’s integration is developer-friendly. They have SDKs (Python, JS) to stream audio. For Azure deployment, it’s just an outgoing connection to Deepgram’s cloud. They also offer an on-premises or private cloud option if needed (enterprises can deploy Deepgram’s engine on their own servers or even at edge). This might be considered if data residency or offline capability is needed, albeit at additional cost.
Pricing: Deepgram’s pricing is usage-based. As of last known info, it was roughly $1.25 per hour for their advanced model (Nova) and lower for their base model. They also have a free tier (e.g. some amount of hours free). If each user uses ~5 hours/day, that’s ~150 hours/month; at $1.25/hour, that’s $187.5 per user per month – which is high if many users. But Deepgram often negotiates enterprise pricing and offers volume discounts. They also allow using smaller models or limiting accuracy slightly for cheaper cost (which might not be desirable here since we want high accuracy). We would likely use their best model for highest accuracy given the assistant’s need to understand everything.
Features: Deepgram supports keyword boosting (you can provide a list of domain-specific words to improve recognition) – e.g. if the assistant has a name or if there are specific product names it should catch, this can be useful. It can also do profanity filtering or redact PII from transcripts if needed (maybe relevant if storing transcripts and concerned about sensitive data). Deepgram doesn’t natively handle wake words (again, separate KWS needed). Continuous multi-turn is supported simply by keeping the stream open or re-opening for each turn. We’d likely close the stream after each user utterance is processed (to reset state), or use one continuous stream if we want constant open-mic (but handling turn-taking becomes more complex). Typically, a voice assistant uses a loop: wake -> start streaming -> user speaks -> stop stream when silence -> process -> respond -> listen again.
5. Microsoft Azure Speech to Text: Since we are deploying on Azure, it’s worth considering Azure’s own Cognitive Services Speech API. Azure’s STT is a cloud service that offers both batch and streaming speech recognition with competitive accuracy. Microsoft has invested heavily in speech; their latest models are close to Whisper’s quality. In the AssemblyAI benchmark, Azure STT had ~8.8% WER in English​
assemblyai.com
, only slightly above Whisper. Azure supports Dutch and dozens of other languages natively. Using Azure STT might simplify deployment (no extra infrastructure beyond enabling the service) and potentially cost less if you have enterprise pricing agreements. Pricing for Azure STT is around $1.4 per hour for real-time transcription (approximately $0.02 per minute for standard models; neural models might be slightly more). They also allow custom models (you can adapt to specific terminology by providing data).
Latency: Azure’s streaming transcription (via the Speech SDK) provides real-time results with low latency, similar to others. The SDK can handle continuous conversation mode. One benefit is that the Azure Speech SDK on devices can even do hybrid processing or handle wake words (they have a concept of an embedded keyword spotter for “Hey Cortana” historically, though custom wake words require training a keyword model using their tools).
Integration: Using Azure STT from a Python backend can be done via the Azure Cognitive Services REST API or the SDK. The REST streaming is a bit complex, but the SDK (with WebSockets under the hood) can be used. Since everything is on Azure, latency should be good (especially if your server and Speech endpoint are in the same region).
Features: Azure’s service can do automatic language detection, profanity filtering, and has stability tuning (to balance accuracy vs. latency for interim results). For stop word detection, we’d parse the text ourselves. Azure doesn’t emit a special event for “stop” or allow arbitrary wake word – those are up to the developer.
Note: One advantage of Azure is unified authentication/management since the whole system is on Azure. If using Azure STT and Azure TTS together, we reduce external dependencies. We can also deploy the STT container locally if needed (Azure offers containerized speech services for on-prem deployment if extremely low latency or offline mode is needed).
6. Coqui STT (Mozilla DeepSpeech lineage): Coqui’s STT is an open-source engine descended from Mozilla DeepSpeech. While functional, its accuracy is notably lower than Whisper or the above APIs on conversational speech. It might be useful for simpler tasks or highly constrained commands, but for a general voice assistant we need the best accuracy. Unless we fine-tune Coqui STT on specific data, it likely wouldn’t meet the “very high accuracy” requirement out-of-the-box. (For reference, DeepSpeech had WER ~10-15% on LibriSpeech clean, and worse on spontaneous speech). Coqui STT does support real-time operation on CPU (it’s light) and can be customized, but given Whisper and others surpass it, we probably wouldn’t choose this except if we needed a fully offline, lightweight solution and Whisper was too slow. 7. Other Open-Source STT (Kaldi/Vosk, Wav2Vec2, etc): There are other open alternatives: Vosk (based on Kaldi) offers offline models for many languages including Dutch. Vosk can run on mobile/embedded, which is impressive, but accuracy is moderate – good for commands but not at the level of neural large models. Wav2Vec2 models (e.g. on HuggingFace) can be fine-tuned to reach high accuracy; for example, Facebook’s Wav2Vec2 Large models approach <10% WER on conversational speech. One could fine-tune a multilingual Wav2Vec or use models like XLS-R for multilingual STT. However, doing so requires ML expertise and GPU inference. Whisper is currently the more widely adopted open model because it’s already tuned on huge data. Another interesting research model is Meta’s SeamlessM4T (perhaps what “Sesame” refers to). SeamlessM4T is a multimodal model that can do speech recognition, speech translation, and even speech-to-speech. It supports nearly 100 languages for STT​
about.fb.com
. However, using it in production is non-trivial – you’d need to run the model (which is large) on GPU, and its quality, while good for translation tasks, may not surpass Whisper for pure transcription. Also, its TTS output (speech-to-speech) is generic sounding and not emotion-rich. So, while cutting-edge, it might not meet the “emotion and low latency” criteria for TTS (discussed later). STT Summary: For the best accuracy and low latency, AssemblyAI’s Universal or OpenAI Whisper are top choices, with Deepgram and Azure right behind. If budget allows and ease-of-integration is key, using a managed API like Deepgram or AssemblyAI will save engineering time. If minimizing cost and maintaining control is important, running Whisper (Large-v2) on an Azure GPU instance is viable – it gives great accuracy in English and Dutch with only the cost of the VM (and you avoid per-request fees). You could even use a smaller Whisper model for faster real-time and then re-process with Whisper-large for accuracy if needed for logs. In terms of continuous dialogue, all these solutions can work: you’d implement a loop that opens a stream when user speaks and closes on silence. None of these have built-in wake word – that will be handled separately (next section). Table 1 below compares key attributes of these STT options: Table 1 – STT Solutions Comparison

STT Solution	Latency (Streaming)	Accuracy (WER)	Multilingual Support	Wake Word	“Stop” Command Detection	Integration Complexity	Pricing (approx)
OpenAI Whisper (self-hosted)	~Realtime on GPU; chunked (200–500ms delay for partials if using mod)	~7.9% WER (EN)​
assemblyai.com
​
assemblyai.com
 – Excellent; Dutch supported	98 languages, auto-detect input	No built-in (external KWS needed)	Detect via transcript (“stop” word appears)	High (manage GPU infra, streaming code)	Free (open source); $0.006/min if using OpenAI API​
assemblyai.com
AssemblyAI Universal (API)	Real-time WebSocket (<300ms typical)	~6.6% WER (EN)​
assemblyai.com
 – Excellent (state-of-art)	English, Spanish, German tested (supports more; likely Dutch)​
assemblyai.com
No (must integrate external KWS)	Transcript monitoring required	Low (simple API/SDK)	Enterprise pricing (contact; roughly $1–2/hr)
Deepgram Nova (API)	Real-time streaming (<300ms)	~9.0% WER (EN)​
assemblyai.com
 – Very Good	Many (English, EU languages incl. Dutch)​
assemblyai.com
​
assemblyai.com
No	Transcript monitoring	Low (API/SDK)	~$1.25/hr (Volume discounts; first 45k min free)
Azure Cognitive STT	Real-time streaming (~200ms)	~8.8% WER (EN)​
assemblyai.com
 – Excellent	100+ languages (English, Dutch, etc.)	Optional (Custom Wake Word via keyword model)	Transcript monitoring (or SDK event)	Medium (use Azure SDK/Websocket)	~$1.4/hr (pay-as-you-go)
WhisperX / SeamlessM4T (open)	Batch alignment (not streaming)	Same as Whisper (for WhisperX); SeamlessM4T ~10% WER for many languages	WhisperX (same languages as Whisper); SeamlessM4T ~100 languages​
about.fb.com
No	N/A (post-processing tool)	High (research code)	Free (open source)
Other Open-Source (Vosk, Coqui STT)	Real-time possible (low resource)	~15%+ WER (lower accuracy)	Models per language (Dutch available in Vosk)	No	Transcript monitoring	Medium (run local service)	Free
Notes: “Latency” indicates how quickly partial/final results are produced once the user speaks. All listed APIs support continuous streaming. None have a built-in wake word engine – that is handled separately (see Wake Word section). Interruption (“stop” phrase) detection is generally done by checking the transcript text for the stop keyword, since none of these STT APIs output a special signal for that. Azure’s SDK might allow registering a keyword model for “stop” separately if desired. Integration complexity is relative: cloud APIs are easier but involve network calls; self-hosted requires ML infrastructure. Accuracy numbers are for English as a reference; actual accuracy for Dutch may vary but Whisper, AssemblyAI, Azure, and Deepgram all have strong multilingual models (likely <10% WER for Dutch broadcast speech, perhaps higher for casual conversational Dutch if not as well represented in training).
Text-to-Speech (TTS) Solutions Comparison
For the voice assistant’s responses, the TTS system needs to generate natural, clear, and expressive speech in multiple languages (at least English and Dutch to start). Key factors include voice quality (human-likeness), available languages/voices, support for emotional tone or speaking styles, latency of generation (fast enough for real-time interaction), and cost (if using a paid API, since 5 hours of speech per day is a large volume of characters). We evaluate major TTS options: 1. Azure Neural Text-to-Speech: Microsoft Azure’s Neural TTS is a top-tier solution with very natural-sounding voices. It offers a large catalog of voices in over 100 languages, including multiple voices for English (US, UK, etc.) and Dutch (e.g. “Dutch (Netherlands) - Colette (Female)” and “Mark (Male)” are available in Azure’s voice list). A standout feature is support for Style and Emotion via SSML. Azure voices can speak in styles like cheerful, sad, excited, shouting, whispering, angry, friendly, etc with appropriate tone​
voicebot.ai
. For example, one can have the assistant respond cheerfully for positive answers or empathetically for a user’s issue. This aligns perfectly with the requirement for emotion-rich speech.
Latency: Azure TTS is quite fast – the API typically returns audio in a second or less for a sentence of text. They also have a streaming endpoint that can begin returning audio while rendering longer texts. The latency is low enough for interactive use (the user might experience a ~1 second pause before the assistant’s voice starts, which is reasonable).
Quality: The neural voices are high quality (nearly indistinguishable from human in many cases, with proper intonation and clear pronunciation). Microsoft continually updates and adds new voices and even allows custom voice creation if needed (record a voice and create a custom TTS voice model, though that’s advanced and requires a lot of data).
Multilingual: Azure covers Dutch fully, so we can have the assistant speak Dutch responses with an appropriate Dutch voice. It can also handle other languages if we expand (French, Spanish, etc.).
Cost: Pricing is typically based on characters. Roughly, neural TTS is ~$16 per 1 million characters. 5 hours of spoken audio is roughly 5 hours * 3600 sec/hour * ~15 chars/sec (assuming ~150 wpm, ~12 characters per word including space) ≈ 270,000 characters. That’s 0.27 million chars, which costs about $4.3 per user per day. Over a month (~30 days), that’s $129 per user. If we have many users, this could be significant. Azure might offer bulk pricing or an S0 tier discount. Self-hosting an open model might save costs at the expense of quality or dev time.
Integration: Very straightforward using the Azure Speech SDK or REST API. We send text and get an audio stream or file (Wave or Ogg). We can even use Azure’s JavaScript SDK on the front-end to generate speech directly in the browser, but since we want to funnel through our backend (for logging and because we may choose other TTS), we’ll likely call it from Python. Ensuring we cache or reuse TTS results for identical prompts could save cost (e.g. static responses), but in conversational AI most outputs differ.
2. Play.ht API: Play.ht is a TTS service known for ultra-realistic voices. They provide 700+ voices in 140+ languages and accents​
play.ht
, which is one of the largest libraries available. These voices include various styles (narration, conversational, characters, etc.) and even some celebrity likeness voices (depending on their licenses). Play.ht emphasizes emotional expression – their voices are designed to capture emotions and intonation from text, using advanced AI models. They have an API and mention “low latency Text to Speech API”​
play.ht
, suggesting it’s geared for real-time applications.
Quality: Play.ht’s top voices are comparable to or even surpass standard Azure/Google voices in naturalness. They often showcase voices that sound truly human (with breaths, pauses, emphasis that feel natural). This would give the assistant a very engaging presence.
Emotions & Variety: While Play.ht voices are high-quality, it’s not clear if developers can explicitly control emotions (like selecting an “angry” tone on the fly). More likely, you choose a voice that inherently has a certain style (e.g. a cheerful voice actor or a sad tone voice). They categorize voices by use-case (e.g. “narrative, customer support, excited”) which implies you pick a voice fit for the context. Some voices might respond more neutrally, others more enthusiastically. ElevenLabs, a close competitor not explicitly listed, has a single voice that can dynamically express emotion via the text content, but doesn’t give explicit emotion tokens. Play.ht likely is similar – you might add punctuation or descriptive cues in the text to influence the delivery. For fine-grained control, Azure or Coqui with SSML might be better.
Latency: The mention of low-latency API suggests you can get audio quickly. It’s likely not as immediate as Azure’s streaming (since Play.ht might generate high-quality audio with more complex models). But for short responses (a sentence or two), it should be within 1-2 seconds. That is acceptable for a voice assistant (similar to Alexa’s typical response delay).
Multilingual: 142 languages and accents are supported​
play.ht
, which is excellent. Dutch is almost certainly among them. They might not have as many Dutch voice options as Azure (which typically has 2–3). But even one good Dutch voice is enough for our needs.
Cost: Play.ht’s pricing is not fully public; they offer subscription plans and possibly usage-based pricing for API. For heavy use (hours per day), it could become costly. For example, if they charge per character similar to Azure or Amazon, it might be in the same ballpark or higher (since they market premium quality). If budget is a concern, Play.ht might end up expensive compared to running an open-source TTS. However, if voice quality is the top priority for user experience (e.g. a very life-like persona), it could be worth it for a smaller number of users or a premium tier.
Integration: They provide a simple REST API where you send text and choose a voice, and you get back an audio URL or binary. We would need to buffer the audio to play it. There might not be a built-in streaming of audio bytes while it’s generated (Azure and Google have an edge with HTTP chunked transfer for long text). But since assistant utterances are typically one or two sentences, we can wait for full audio.
3. Coqui TTS / Mycroft Mimic 3: Coqui TTS is an open-source project building state-of-the-art TTS models (the successor to Mozilla TTS). Mycroft’s Mimic 3 is a ready-to-use TTS engine built on Coqui models, focusing on privacy and offline use. Mimic 3 comes with 100+ voices in 25 languages (including multiple English voices and many others like Dutch, German, French, etc.)​
voicebot.ai
. The quality of these voices varies (some are high-quality neural voices, some are lower-quality or very robotic depending on the data). However, many are quite good, and being offline and open-source means we can customize.
Quality: The best Coqui/Mimic3 voices approach the naturalness of cloud services, especially for English. For Dutch, there are at least a couple of voices. They may not be as smooth or emotive as Azure’s, but they are serviceable. There is also the possibility to train or fine-tune a voice if we have recordings (for a custom company persona, etc.).
Emotion: Most open TTS models do not have explicit emotion control unless they were trained with such. Mimic3 supports SSML partly (you can adjust speaking rate, pitch, volume, etc., and even switch voices mid-sentence for dialogues)​
voicebot.ai
. But it doesn’t have pre-built “angry” or “sad” styles. That said, one could select a voice that inherently sounds upbeat or one that sounds calm for different modes. Also, some research models (like using global style tokens or multimodal emotion embeddings) could be integrated if needed, but that’s complex.
Latency: A well-optimized TTS model like FastSpeech2 or Tacotron2 + HiFiGAN can generate speech faster than real-time on a CPU, and much faster on GPU. Mimic3 is designed to run even on small devices (Raspberry Pi) albeit slower. On a server-grade CPU it might generate a second of speech in under 1 second. On a GPU, definitely under real-time. So latency can be very low, especially if we generate sentence by sentence. We can even stream out audio chunks as they are generated if using certain architectures (some TTS can generate on the fly). Mimic3 likely generates the whole waveform then returns it (which is still quick).
Cost & Deployment: Being open-source, the cost is just the compute on Azure. Running a container with Mimic3 is possible (they provide Docker images). If we have many concurrent users, we might need multiple instances or a GPU for TTS to handle many requests simultaneously. But even a single CPU might handle a few streams concurrently if they’re not all speaking at once. The benefit is cost predictability – no per-character fee.
Integration: We can integrate Mimic3 by running it as a local HTTP server (it has an API) or by calling its library from Python and getting back an audio buffer. This is more effort than calling a ready API, but it gives full control. Also, no internet dependency means we can even generate voice offline or in a private environment.
4. ElevenLabs (not listed, but similar to Play.ht): ElevenLabs is worth a quick mention – it’s a very popular TTS known for its extremely expressive and realistic voices (often used for content creation). It supports English very well, and recently added multilingual support (it can read text in several languages with an appropriate accent). It allows voice cloning – one can create a custom voice from a few minutes of audio. For a voice assistant persona, this could be interesting (you could clone a specific actor’s voice or any unique voice with permission). Emotional expression with ElevenLabs is implicit but very effective; the model will express the sentiment found in text (question, exclamation, etc.) quite naturally. Pricing is similar to others (they have subscription tiers that include so many characters per month, e.g. $5 for 100k chars, $30 for 500k, etc., and then overages). Given heavy use, cost could escalate. Integration is via API (they return an audio file quickly). If the user is aware of ElevenLabs and Play.ht, they likely considered them similar category – for our purposes, either could be a top-quality TTS API. 5. Other open-source TTS (e.g. Tortoise, FastPitch, Bark): There are research models like Tortoise-TTS (very high quality, but extremely slow – not suitable for realtime, it’s more for offline generation), or Bark by Suno (multilingual, can even include non-speech sounds, but also slow and not easily controllable yet). FastPitch + HiFiGAN (NVIDIA) or VITS models are fast and can be very good – in fact, Mimic3 likely uses some variant of VITS for many voices. If we had a specific need (say a unique language or a specific style), we could train a model using NVIDIA NeMo toolkit. But this is a lot of work and only worth it if no existing voice meets our needs. TTS Summary: For an emotion-rich, multilingual, low-latency voice, the leading choices are Azure Neural TTS (for full control over style and easy multi-language) or a high-quality provider like Play.ht/ElevenLabs (for possibly even more natural-sounding voices out-of-the-box). If controlling cost is critical, using an open-source engine like Mimic 3 (Coqui TTS) on our own server is attractive – we sacrifice some top-tier naturalness and ease of use, but avoid per-use fees and keep data on our servers. A hybrid approach could also work: use open-source TTS for most cases and call premium API for certain special responses where the ultimate quality matters. We must also consider interruptibility: If the user says something while TTS is playing, we need to stop the audio output. This is mostly an application logic issue – whichever TTS we use, we can always stop playback on the client. But if we wanted to stop generation mid-way (e.g. if a response is long and user interrupts, we could stop generating further audio to save resources), that’s easier if we are streaming TTS (like Azure’s API can cancel, or local TTS you can just stop processing). With services that return a whole file (like Play.ht or ElevenLabs), you’d generate the whole thing even if not fully used. This is a minor consideration. Below is a comparison table for TTS solutions: Table 2 – TTS Solutions Comparison

TTS Solution	Latency	Voices & Languages	Emotion/Style Control	Interruptibility	Integration	Cost
Azure Neural TTS	~1s per sentence (streaming available)	400+ voices, 100+ languages (multiple English, Dutch, etc.)	Yes – SSML styles (cheerful, sad, etc.)​
voicebot.ai
; pitch/rate control	High – can stream/cancel mid-output	Easy (REST API or SDK)	~$16 per 1M chars ( ~$4 per 5h speech )
Play.ht	~1–2s for short text (no true stream)	800+ voices in 142 languages​
play.ht
 (very natural, many styles)	Partial – choose voice by style (no explicit SSML emotion, but voices capture emotion)	Medium – no streaming, but can stop playback client-side	Easy (REST API)	Subscription or usage-based (premium)
ElevenLabs	~1s for short text (fast API)	~20+ high-quality voices (mainly English, some multi-lingual adaptability) + custom voice cloning	Implicit – expressive delivery from text (no explicit tags)	Medium – no streaming (stop on client)	Easy (REST API)	Subscription + overage (can be costly for heavy use)
Coqui TTS / Mimic3 (open-source)	<1s per sentence (faster with GPU; runs offline)	100+ voices, 25+ languages​
voicebot.ai
 (quality varies; many decent voices)	Limited – can control rate, volume; switch voices; no pre-built emotions	High – run locally, can stop synthesis anytime	Medium (host your service)	Free (compute cost only)
Google Wavenet TTS	~1s per sentence	220+ voices, 40+ languages (English, Dutch included)	Some – speaking styles for a few voices (news, casual)	High – supports stream API for long text	Easy (API)	~$16 per 1M chars (similar to Azure)
Mimic 3 (Mycroft)	~1s (fully offline)	(Same as Coqui TTS above; open-source)	Same as Coqui (SSML subset)	High (offline control)	Medium (local install)	Free
Notes: Azure and Google are similar in offering robust cloud TTS with multi-lingual support and some style control. We listed Google for completeness – since we’re on Azure, Azure TTS is preferable. ElevenLabs and Play.ht represent the cutting-edge in voice quality (some voices truly sound like a person talking spontaneously). If the “assistant personality” and user engagement are a priority, these might be worth the cost. Open-source solutions like Coqui/Mimic3 give flexibility (you can even modify the voice’s pronunciation dictionary if needed – useful for company names, etc.) and privacy. Latency for all is low enough that the user will have a short wait between their query and the spoken answer. “Interruptibility” mainly refers to our ability to handle barge-in: Azure’s streaming can be stopped, local TTS can be stopped; with others, we generate full audio but we can always stop playback on the device. In practice, that’s sufficient – when user interrupts, we stop playing the audio immediately.
Wake Word Detection Solutions
The wake word engine is what allows the assistant to start listening without a button press. It continually monitors the microphone for a specific keyword (like “Hey Athena” or any chosen name) and, upon detecting it, signals the main app to start the STT stream. Wake word detection should be low-power, highly accurate (few false alarms or misses), and ideally run on the client device (phone) to avoid streaming all audio to the cloud for privacy and efficiency. Below are some options:
Picovoice Porcupine: A popular commercial on-device wake word SDK. It’s lightweight (runs on mobile CPUs), supports custom wake words and even some basic ones in multiple languages. Porcupine is known for high accuracy with low false trigger rate. It can detect multiple wake words if needed. The downside is cost/licensing – it’s free for limited personal use, but for a product you might need a license. It does support Dutch (you can train a wake word with Dutch phonetics or use their predefined ones). Integration: Porcupine has iOS and Android libraries (and even a WebAssembly version for web). So we could embed it in the mobile app/clip to locally listen. Once it triggers, the app would open the voice assistant UI and connect to backend STT. Porcupine’s detection is instantaneous (a few tens of milliseconds after the keyword is spoken).
Mycroft Precise: An open-source wake word detector (used in Mycroft AI). Precise uses a neural network model (simple CNN/RNN) that you train on examples of your wake word and background noise. Quality depends on training data – with sufficient samples it can be quite good. The benefit is it’s free and can be tuned to a custom phrase easily. For example, you could have the wake phrase be the company name or something unique. Running Precise on mobile is possible (it’s Python-based, but could be converted or run in a React Native plugin perhaps). However, Precise might be more heavy on CPU than Porcupine and possibly less optimized. Still, if open-source stack is desired, this is an option. There are community models for “Hey Mycroft” etc., but for our custom phrase we’d collect recordings to train it.
Snowboy (KITT.AI): Snowboy was an older (now discontinued) offline wake word engine. It allowed training models for custom wake words using a few samples. It supports multiple languages (including a pre-trained “Hey Siri” in Dutch and such). Many open-source assistant projects used Snowboy. However, Snowboy is no longer maintained, and on iOS it might be tricky due to 32-bit library issues. If it works, it’s lightweight and could do the job, but Porcupine or Precise have largely superseded it in community projects.
OpenWakeWord: A newer open-source project aiming to provide an efficient wake word engine​
github.com
. It’s in development but shows promise in allowing custom wake word training and on-device inference. Could be worth watching if we want fully open solution with better architecture than Precise.
Hardware/OS built-in: On Android, there is Google’s Hotword detection (for “OK Google”), but that’s not accessible for custom apps. On iOS, there’s no public API for Siri wake word. So we cannot leverage OS-level always-listening due to sandboxing. We must implement our own via the app’s audio.
For our use-case, since the user might launch the assistant via NFC, it implies the app isn’t running until the NFC tag is tapped (which could open the App Clip or web app). If the requirement is to then continuously listen for the wake word, presumably the app would remain open (maybe in background?) listening. On mobile, continuous background listening is tricky (OS limits). However, if the assistant is meant to be used actively while the app is open, we can do wake word as long as the app is in foreground or using a background audio mode. Recommendation for Wake Word: Use Porcupine if licensing is feasible – it’s high accuracy and easy (just integrate their SDK, and either use a pre-built wake word or get a model trained for our keyword). Alternatively, if we want everything open-source, use Precise with a custom model. In either case, we’ll have the wake word engine running on the device; when it fires, we open a WebSocket to the backend for STT. We should also include a tap-to-talk fallback in case the wake word fails or for accessibility. Wake word engine typically also provides a “silence” or “VAD” (voice activity detection) mechanism to know when to stop listening after the wake word or user query. Often, one combines: wake word triggers -> start STT -> STT (or a separate VAD) monitors silence -> if silence > e.g. 1s, conclude user finished -> stop STT and start processing response. We can use simple energy threshold VAD or WebRTC VAD for silence detection on the server or client side. Stop Word vs Wake Word: The “stop” command (to interrupt the assistant) could be handled similarly to a wake word. We might implement a small keyword spotter for the word “stop” always listening when TTS is playing. For example, Picovoice has another product “Porcupine (multiple keywords)” or even a separate engine Porcupine can detect multiple phrases concurrently​
rhasspy.readthedocs.io
. We could configure it with both the wake word and the word “stop” as triggers. Then:
If wake word is detected while idle, we start interaction.
If “stop” is detected while TTS is speaking, we halt the TTS. (We should ensure the TTS voice saying “stop” in normal conversation doesn’t accidentally trigger it – but if our wake word is unique enough or we only arm the “stop” detection during TTS playback, it should be fine.)
Alternatively, we rely on the user saying “stop” and that being transcribed by the STT. But STT only runs when user is in session, and if the assistant is talking, STT might not be running. So having a separate stop-word detector running during TTS output is a good design for true interruption capability. It could be the same module as wake word (just listening for a different keyword). Integration complexity for wake word: We will need to integrate native code or libraries into the mobile app (for App Clip, the size is limited, but Porcupine is only a few MB). In React Web (if it’s purely a web app running in Safari after NFC via App Clip?), wake word detection via WebAssembly might be possible but not sure about constant mic access without user gesture in web. App Clips can use the microphone though. Possibly the voice assistant front-end might be a React web app embedded in the App Clip container. If purely web, continuous listening might face browser security restrictions (most browsers require user gesture to start microphone, and won’t allow always-listening in background). If a native mobile container is used (like React Native or App Clip + WebView), we could handle it better. For now, assume we can make it work using the above libraries. Wake Word Comparison (briefly):
Porcupine (Picovoice): Highly accurate, low-latency (runs at ~is realtime on device), multi-platform. Not open-source, but free for limited use. Supports custom wake phrases. Likely the most robust choice.​
rhasspy.readthedocs.io
Precise (Mycroft): Open-source, needs training data, moderate accuracy (depends on model quality). Python-based (would need porting to mobile, but possible via TensorFlow Lite maybe).
Snowboy: Quick setup with a few samples, moderate accuracy, unmaintained but might still function for some (would need bridging C++ lib into app).
Others: Google’s SpeechCommands models could be repurposed (there is a dataset for the word “stop” or “go” etc., but not for arbitrary trigger phrase). Also “ALexa KWS” research models exist but not readily available.
Given the focus is on best solutions, Porcupine stands out for wake word. Rhasspy docs show it as one of the recommended systems alongside Precise and Snowboy​
rhasspy.readthedocs.io
.
Integration Architecture
Bringing it all together, the architecture will look like this: Figure: End-to-end voice assistant architecture, showing wake-word detection on device, streaming STT to backend, LLM processing, and TTS back to user.​
developer.nvidia.com
​
developer.nvidia.com
 In the client (mobile app or web app), the wake word engine runs continuously. Once the user says “Hey Assistant” (for example), the device detects it (on-device, no cloud needed yet). The React front-end (or native wrapper) then opens a WebSocket connection to the Python backend (hosted on Azure, possibly an Azure Web App or Function with support for WebSockets). The WebSocket carries audio data: the front-end starts capturing microphone audio and streaming it to the backend in small chunks (e.g. 100 ms packets). On the Python backend, we have a websocket endpoint that receives these audio chunks. This audio is fed into the chosen STT system. For instance: if using Deepgram’s API, the Python service could simply pipe the audio to Deepgram’s own WebSocket and relay transcripts back; if using Whisper locally, the Python code would buffer audio and run the model incrementally. The STT yields partial transcripts and final transcript when user finishes. Partial transcripts can be used for real-time feedback (though our use-case might not need to display partial text, we mostly need final command). Critically, if a partial or final transcript equals the word “stop” (and the assistant was speaking), the backend can signal an interruption. Once the user’s utterance is fully transcribed (or the user stopped speaking as detected by silence or the STT end-of-speech), the backend sends the text to the conversation manager. This could be an LLM (like an OpenAI GPT-4 via API) or a custom intent handler. For a general assistant, likely we’d send it to an LLM (with some prompt engineering to include context from Pinecone vector DB as needed, etc.). Or route it: if the user’s request matches certain actions (like “turn on the light”), bypass LLM and execute an action. This is the “dispatcher for LLM or action routing” mentioned. The LLM or logic produces a text response (or action). That response text is then passed to the TTS engine. The Python backend either calls the TTS API (Azure/Play.ht/etc.) or generates via local TTS. The resulting audio stream or file is then sent back to the client. If using WebSocket, we can send the audio data through it (possibly in chunks if streaming). Alternatively, we might send a URL or ID for the client to fetch the audio. Simpler is to send the binary data over the socket since it’s already open. The client receives the audio and plays it through the speaker, letting the user hear the assistant’s answer. Now, for continuous dialogue, after the assistant stops talking, we likely keep the session open and listen for the user’s next utterance. Possibly we require the wake word again or we could implement a short window in which the wake word is not needed (like follow-up mode). A common approach is: once awake, the assistant stays active for a certain timeout (say 30 seconds of inactivity) or until the user explicitly says a “goodbye” or the app is closed. We should ensure the wake word detector is disabled while conversation is ongoing to prevent accidental retriggers (or maybe change the wake word during conversation, e.g. only “stop” is listened for). Throughout, each interaction’s transcript and response are stored: The backend can save {userID, mode, transcript, response, timestamp} to MongoDB for history. Additionally, it can take the embedding of the conversation (e.g. embed the text of user query or important facts from it) and upsert into Pinecone for long-term semantic memory (so the assistant can recall things said previously by searching vectors). Each user would have their own namespace in Pinecone to keep private data separate from work data, etc. Interruption Handling: If the user says “stop” while TTS is playing, the wake-word/stop-word detector on device could directly stop the audio playback (client-side) and also inform backend. Alternatively, if the user just speaks over it with the wake word, the system might hear the wake word and immediately cut off TTS and start listening. Implementing barge-in robustly might involve echo cancellation so that the mic doesn’t feed the assistant’s own voice into STT. Some advanced setups send the TTS audio waveform to the ASR to ignore, or use half-duplex (don’t listen while talking). A simpler approach: when TTS starts, we lower sensitivity of wake word to avoid false triggers from the assistant’s voice, but still allow a loud “Hey Assistant” from the user to trigger. This is an area to fine-tune in practice. For now, assume either the user will say “stop” or wait. Next, we provide some recommendations for combining these components:
Recommended Top Combinations
Based on the above comparisons, here are the top 3 recommended STT+TTS (plus wake word) combinations for this project, balancing accuracy, latency, cost, and complexity: Option A – Open-Source Stack (Whisper + Mimic3 + Precise): This combination favors control and cost savings. Use OpenAI Whisper (large-v2) deployed on an Azure VM for STT, Mycroft’s Precise for wake word “hey assistant” on device, and Mimic 3 TTS running on the Azure backend for voice output.
Pros: All components are self-hosted or open-source (no per-use fees, data stays within our system). Whisper gives top-tier accuracy across English and Dutch​
assemblyai.com
​
assemblyai.com
. Mimic3 provides a variety of voices offline (we can pick a decent English and Dutch voice, and even switch voices per user preference). We can fine-tune models if needed (e.g. improve Whisper for domain-specific jargon, or record a custom voice for TTS). Also, this stack can be customized – e.g. add a new language by training a model, or adjust the wake word easily.
Cons: Integration and maintenance complexity is highest here. We need to manage a GPU server for Whisper (and ensure it can handle possibly concurrent streams – Whisper large might only handle a couple in real-time on one GPU). We also accept slightly higher latency; Whisper processing might introduce ~1s delay for longer utterances (though it’s streaming internally, the final result might come a bit after speech end). Mimic3’s voice, while good, may not be as emotionally natural as Azure or Play.ht’s premium voices – the user experience might be a bit more robotic. Also, Precise wake word may need significant training data and tuning to be reliable. If not tuned well, false triggers or misses could frustrate users. Overall, this option is best if avoiding recurring costs is a priority and if the team has ML expertise to support it. It’s essentially our own Siri stack on Azure.
Option B – Hybrid Cloud Stack (Deepgram STT + Azure TTS + Porcupine): This combination mixes a specialized STT API with a reliable TTS API, focusing on performance and multilingual support. Deepgram’s streaming STT handles the user’s voice input, and Azure’s Neural TTS handles responses, while Porcupine on-device listens for the wake word.
Pros: Both STT and TTS are production-grade and highly optimized. Deepgram will yield fast, accurate transcripts with minimal effort on our part (just an API key and endpoint). Azure TTS will give us extremely natural voices with emotional tone control​
voicebot.ai
 and full Dutch support. We can even use Azure to have a Dutch voice speak Dutch responses and an English voice speak English, if our assistant switches languages (Azure can detect language of text and choose voice accordingly via SSML). Porcupine ensures reliable wake-word detection with industry-leading performance (for example, very low false accept rate, ensuring the assistant isn’t accidentally triggered). Integration complexity is moderate: we have to tie together two external services, but both have good SDKs. This option would yield a high-quality user experience – the assistant will understand well and speak eloquently.
Cons: Cost could be significant. Deepgram at heavy usage (e.g. 150 hours per user per month) and Azure TTS (millions of chars) together might run into tens or hundreds of dollars per user monthly, which may not scale if userbase grows. However, those usage numbers are on the high end; actual usage might be lower (few users talking 5h/day is a lot). We could mitigate cost by only streaming when needed (not keeping the mic open unnecessarily) and possibly by compressing audio (Deepgram charges less for narrowband audio, etc.). Another con: reliance on external cloud services means if internet is down or their service has an outage, the assistant might fail. Also, data (audio) does get sent to Deepgram servers (privacy considerations – though they allow opting out of data retention). For enterprise use, that might be fine with agreements in place.
Option C – All-in Azure (Azure STT + Azure TTS + Porcupine): This leverages Azure Cognitive Services for both speech recognition and synthesis. With this, we essentially use Microsoft’s stack end-to-end, except wake word (Porcupine or possibly a custom Keyword model in Azure Speech SDK).
Pros: Simplified DevOps: Using Azure for STT and TTS means a single platform – easy auth, unified SDK, and Azure reliability. We can deploy this within the same region for minimal latency. Accuracy and quality are high: Azure’s STT is on par with others for English and supports Dutch well (with language auto-detection if needed). The TTS voices, as noted, are excellent with emotional styles. Additionally, Azure’s Speech SDK has features like adaptive beamforming, noise suppression, etc., which could help in mobile scenarios. If needed, Microsoft even provides wake word capabilities: one can create a custom wake word model using the Speech service (this requires recording samples and training via their portal). This model can then be used in the Azure Speech SDK on the device to detect the wake word (so we could potentially use Azure for everything, though Porcupine is probably easier). Storing data in Azure’s ecosystem (CosmosDB instead of Mongo, perhaps, and maybe Azure Cognitive Search or an Azure vector DB instead of Pinecone) could also be considered, but that’s beyond scope – we can stick to MongoDB Atlas and Pinecone if preferred.
Cons: Azure STT, while good, might not be as straightforward as Deepgram in terms of real-time socket use; we might need to use the SDK in streaming mode on the backend (which is doable). It’s also not the absolute bleeding edge in accuracy (Whisper/Assembly might edge it out slightly, but likely not enough to matter in practice). Cost is similar to others – STT ~$1.4/hr, TTS ~$4 per 5h, so maybe ~$10 per user-day for heavy use. Volume or enterprise discounts could lower that. Another consideration: if the assistant’s LLM is also OpenAI, we might end up calling Azure OpenAI service or OpenAI API – mixing Microsoft and OpenAI is fine (Microsoft hosts OpenAI too). In fact, Azure OpenAI could host the LLM. So an entire Microsoft-centric solution is viable if that alignment is beneficial (one vendor, easier compliance).
All the above options assume mobile on-device wake word (Porcupine or equivalent). If we absolutely couldn’t run a wake listener on device, an alternative is having the app send a low-bitrate audio stream to the cloud and using something like WebRTC with VAD to trigger listening. But that’s not ideal and wasteful. So we stick with on-device KWS. Recommendation: If budget and fastest time-to-market are priorities, Option B (Deepgram + Azure TTS + Porcupine) is a strong choice – it minimizes development of complex ML pipelines and ensures high quality. If data privacy or long-term cost is more important, Option A (Whisper + Mimic3) might be chosen, but expect more engineering effort. Option C (all-in Azure) is a happy medium if one prefers a single cloud vendor solution and is okay with Azure’s pricing and slightly less “tweakability.” In practice, we could even mix and match further: e.g., Whisper (self-hosted STT) with Azure TTS (if we trust our STT more or to save some cost on STT since Whisper is free). That could be a 4th combo – Whisper + Azure TTS + Porcupine – giving no STT cost, only TTS cost. The downside is managing Whisper infra. But that is actually quite attractive too for multi-language (Whisper is excellent for multilingual). So let’s mention that as well: Option D – Whisper + Azure TTS: Use Whisper (perhaps via OpenAI API if not self-hosted) for speech recognition and Azure for speech synthesis. This yields low cost on STT (OpenAI API $0.006/min is extremely cheap, or self-host cost of a GPU) and high quality on TTS. The latency might increase a bit if not streaming Whisper, but if user utterances are short, it’s fine. Storing in MongoDB & Pinecone: All options output text transcripts and we can easily take those strings and store them. The vector storage in Pinecone will use an embedding model (likely OpenAI’s text-embedding-ada or similar) to encode either each user query or key information to enable retrieval. This is independent of the voice pipeline – it’s part of the text/LLM pipeline. So, whichever STT we use, we get text and then we do: pinecone.upsert(id=userID_timestamp, vector=embedding_of_text). And for Mongo, we just insert the record.
Code Samples for Integration
Finally, to illustrate how the integration works, below are simplified code snippets for the Python backend (using FastAPI for WebSocket and pseudocode for STT/TTS integration) and the React front-end (using Web APIs to capture audio and play responses). These are basic examples to show the flow: Python Backend (FastAPI + WebSocket): This example uses Deepgram’s Python SDK for STT and Azure’s SDK for TTS. (In reality you’d pick one STT; we use Deepgram here for demo.)
python
Kopiëren
# backend.py (FastAPI app)
from fastapi import FastAPI, WebSocket
import asyncio
from deepgram import Deepgram       # Deepgram STT SDK
import azure.cognitiveservices.speech as azure_speech  # Azure TTS SDK

app = FastAPI()

# Initialize STT and TTS clients (with respective API keys)
DEEPGRAM_API_KEY = "YOUR_DEEPGRAM_KEY"
dg_client = Deepgram(DEEPGRAM_API_KEY)
# Azure Speech config for TTS
AZURE_SPEECH_KEY = "YOUR_AZURE_KEY"
AZURE_REGION = "westeurope"  # example region
speech_config = azure_speech.SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_REGION)
speech_config.set_speech_synthesis_voice_name("en-US-JennyMultilingualNeural")  # voice that can speak English & Dutch
speech_synthesizer = azure_speech.SpeechSynthesizer(speech_config=speech_config, audio_config=None)

@app.websocket("/audio")
async def audio_stream(ws: WebSocket):
    await ws.accept()
    # Start Deepgram's live transcription websocket
    dg_socket = await dg_client.transcription.live({'punctuate': True, 'interim_results': False})
    # Coroutine to forward Deepgram transcripts to processing
    async def process_transcripts():
        async for transcript in dg_socket:
            if transcript.get('is_final'):
                text = transcript.get('speech', {}).get('transcript', '')
                text = text.strip()
                if text.lower() in ["stop", "cancel"]:
                    # Stop current TTS if any (not shown: manage TTS playback state)
                    print("Received stop command")
                    # We could signal the client to stop playing audio here
                    continue
                print(f"User said: {text}")
                # Pass text to LLM or action handler (not shown)
                response_text = handle_user_query(text)  # your function to get answer from LLM
                # Synthesize TTS audio from response
                tts_stream = speech_synthesizer.speak_text_async(response_text).get() 
                audio_data = tts_stream.audio_data  # binary audio bytes (WAV)
                # Send audio bytes back to client
                await ws.send_bytes(audio_data)
                # Save to MongoDB and Pinecone
                save_chat_to_mongo(user_id="123", query=text, answer=response_text)
                save_embedding_to_pinecone(user_id="123", text=text + " " + response_text)
    # Start listening for transcripts
    task = asyncio.create_task(process_transcripts())
    try:
        # Receive audio from client and send to Deepgram
        while True:
            data = await ws.receive_bytes()
            await dg_socket.send(data)
    except Exception as e:
        print("WebSocket connection closed:", e)
    finally:
        await dg_socket.finish()
        task.cancel()
In this code, when a WebSocket client connects to /audio, we create a Deepgram live transcription connection. We forward all incoming audio bytes from the client to Deepgram. We asynchronously listen for final transcripts from Deepgram. When a final transcript is received (i.e., user finished speaking), we check if it’s “stop” to handle interruption. Otherwise, we process the user query (e.g., call an LLM or any logic – not expanded here) and obtain a response_text. Then we use Azure’s SDK to synthesize speech from that text. We send the resulting audio bytes back to the client via the WebSocket. Finally, we log the conversation to Mongo and Pinecone (pseudo functions save_chat_to_mongo and save_embedding_to_pinecone are placeholders for those database operations). This demonstrates handling continuous dialogue in a single WebSocket session: the server can handle multiple turns sequentially (after sending TTS, it goes back to waiting for more audio from the client). React Frontend (Using Web Audio API + WebSocket): This example uses browser APIs to capture microphone audio and play audio:
jsx
Kopiëren
// Frontend pseudocode (React component)
import { useEffect, useRef } from 'react';

function VoiceAssistant() {
  const socketRef = useRef(null);
  const audioContextRef = useRef(null);

  useEffect(() => {
    audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
    // Initialize WebSocket to backend
    socketRef.current = new WebSocket("wss://your-backend-url/audio");
    socketRef.current.binaryType = "arraybuffer";  // expecting binary audio data back

    // When TTS audio comes in from server:
    socketRef.current.onmessage = (event) => {
      if (event.data instanceof ArrayBuffer) {
        const audioData = event.data;
        // Play the received audio buffer
        audioContextRef.current.decodeAudioData(audioData, (buffer) => {
          const source = audioContextRef.current.createBufferSource();
          source.buffer = buffer;
          source.connect(audioContextRef.current.destination);
          source.start(0);
        });
      } else {
        console.log("Received text message:", event.data);
      }
    };

    // Start microphone capture and send to server
    navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
      const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
      mediaRecorder.addEventListener('dataavailable', (e) => {
        // send audio chunks to backend
        if (socketRef.current.readyState === WebSocket.OPEN && e.data.size > 0) {
          e.data.arrayBuffer().then(buf => {
            socketRef.current.send(buf);
          });
        }
      });
      mediaRecorder.start(250); // capture in 250ms chunks

      // (Optional) Handle wake word via WebRTC or Porcupine WASM here. For simplicity, assume recording starts on a button or wake event.
    });

    return () => {
      // cleanup on unmount
      socketRef.current.close();
    };
  }, []);

  return (
    <div>
      <h1>Voice Assistant</h1>
      {/* UI could show transcription or status here */}
    </div>
  );
}
In this front-end code, when the component mounts, we create a WebSocket connection to the backend’s audio endpoint. We also request microphone access via getUserMedia. We use a MediaRecorder to record audio in small chunks (250ms). On each dataavailable event, we convert the audio blob to an ArrayBuffer and send it over the WebSocket. The server will process it and eventually send back audio data (e.g., WAV or raw PCM). We listen for onmessage on the socket – if binary data arrives, we feed it into the Web Audio API (decodeAudioData) and play it through an AudioBufferSourceNode. This will output the assistant’s speech through the user’s speakers. We could also handle text messages (not used in our backend, but if the server sent interim transcripts or other info, we could display them). For wake word, in this snippet we assume the recording is started manually or already running. In a real app, you might start MediaRecorder only after detecting the wake word. If implementing Porcupine in the browser, you’d use its WebAssembly and microphone input to detect the keyword, then initiate the connection. Note: On iOS Safari, Web Audio and getUserMedia work within certain constraints (need user gesture to start, cannot run indefinitely in background). An App Clip might need to keep running as long as user interacts. Ensuring the mediaRecorder runs and sends continuously is key. Also, audio format: we chose 'audio/webm' for MediaRecorder because it’s widely supported. The backend must handle that (Deepgram does support WebM/Opus, Azure STT would too if we used it, or if using Whisper, we’d need to decode Opus to PCM). Alternatively, we could record raw PCM via Web Audio API and send that (but that’s more involved). Finally, the front-end should also provide a visual cue or text for what the assistant heard (e.g., show the transcribed text) and maybe the response text while speaking. That can be done by sending those pieces as textual messages over the socket or a separate channel. The code focuses on audio streaming.
Conclusion
In summary, building a cloud voice assistant with continuous dialogue on Azure is feasible today by combining advanced STT and TTS services. For STT, OpenAI’s Whisper (either via API or self-hosted) and AssemblyAI/Deepgram lead in accuracy (WER ~6-8%​
assemblyai.com
) and support multilingual input. For TTS, Azure Neural and Play.ht/ElevenLabs offer highly natural, low-latency speech with emotional expression, while open alternatives like Coqui’s Mimic 3 provide cost-effective offline voices. A robust wake word system such as Picovoice Porcupine on the mobile device ensures a seamless hands-free experience. By leveraging streaming WebSocket connections, the system can achieve real-time transcription and prompt responses – a critical factor since even 200ms delays are noticeable in conversation​
developer.nvidia.com
. We compared multiple solutions in terms of latency, accuracy and cost in tables above, and recommended combinations that best meet the project’s needs. Final recommendation: Use Deepgram for STT (for its real-time accuracy and easy integration) paired with Azure Neural TTS (for its expressive, multi-language voices) and handle wake-word with Porcupine SDK on device. This mix should deliver an assistant that feels responsive and natural in both English and Dutch. All conversation data can be logged to MongoDB and vectorized into Pinecone to continually improve the assistant’s contextual awareness. With the provided architecture and code templates, the development team can implement and iterate on the voice assistant, tuning each component (e.g. adjusting wake-word sensitivity, choosing voices or STT models per user mode) to achieve an optimal user experience.